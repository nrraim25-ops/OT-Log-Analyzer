{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0vrhfNs9AQXahVbSD4pvt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nrraim25-ops/OT-Log-Analyzer/blob/main/BasicLogAnalyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKrrgqrdeDOH",
        "outputId": "e75a082b-728f-4c5e-9342-4d8a0079a7dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Analyzing log file: sample_ot.log\n",
            "Parsed 8 log entries\n",
            "============================================================\n",
            " OT LOG ANALYZER REPORT\n",
            "============================================================\n",
            "\n",
            " SUMMARY:\n",
            "  Total Entries: 8\n",
            "  Error Rate: 25.00%\n",
            "  Warning Rate: 12.50%\n",
            "\n",
            " DEVICE BREAKDOWN:\n",
            "  PLC: 4\n",
            "  Firewall: 2\n",
            "  RTU: 1\n",
            "  HMI: 1\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import json\n",
        "from collections import Counter, defaultdict\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "class OTLogAnalyzer:\n",
        "    \"\"\"Log Analyzer for OT/SCADA systems - Perfect for your Siemens project\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.log_entries = []\n",
        "        self.statistics = {\n",
        "            'total_entries': 0,\n",
        "            'error_count': 0,\n",
        "            'warning_count': 0,\n",
        "            'device_counts': Counter(),\n",
        "            'error_types': Counter(),\n",
        "            'hourly_activity': defaultdict(int)\n",
        "        }\n",
        "\n",
        "        # Precompile regex patterns\n",
        "        self.patterns = {\n",
        "            'timestamp': re.compile(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}'),\n",
        "            'ip': re.compile(r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b'),\n",
        "            'severity': re.compile(r'\\b(DEBUG|INFO|WARNING|ERROR|CRITICAL)\\b'),\n",
        "            'scada': re.compile(\n",
        "                r'(?P<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\s+'\n",
        "                r'(?P<device>PLC|RTU|Firewall|HMI)\\s+'\n",
        "                r'(?P<severity>DEBUG|INFO|WARNING|ERROR|CRITICAL)\\s+'\n",
        "                r'(?P<message>.*)'\n",
        "            )\n",
        "        }\n",
        "\n",
        "    def parse_log(self, filepath):\n",
        "        \"\"\"Parse log file and extract structured information\"\"\"\n",
        "        print(f\" Analyzing log file: {filepath}\")\n",
        "\n",
        "        with open(filepath, 'r') as f:\n",
        "            for line_num, line in enumerate(f, 1):\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "\n",
        "                # Try to parse as SCADA log first\n",
        "                entry = self._parse_scada_log(line)\n",
        "                if not entry:\n",
        "                    # Fall back to generic parsing\n",
        "                    entry = self._parse_generic_log(line, line_num)\n",
        "\n",
        "                if entry:\n",
        "                    self.log_entries.append(entry)\n",
        "                    self._update_statistics(entry)\n",
        "\n",
        "        print(f\"Parsed {len(self.log_entries)} log entries\")\n",
        "        return self.log_entries\n",
        "\n",
        "    def _parse_scada_log(self, line):\n",
        "        \"\"\"Parse SCADA-specific log format\"\"\"\n",
        "        match = self.patterns['scada'].search(line)\n",
        "        if match:\n",
        "            entry = match.groupdict()\n",
        "            entry['line'] = line\n",
        "            return entry\n",
        "        return None\n",
        "\n",
        "    def _parse_generic_log(self, line, line_num):\n",
        "        \"\"\"Generic log parser with key=value extraction\"\"\"\n",
        "        entry = {'line_number': line_num, 'raw': line}\n",
        "\n",
        "        # Extract key=value pairs\n",
        "        pairs = line.split()\n",
        "        for pair in pairs:\n",
        "            if '=' in pair:\n",
        "                key, value = pair.split('=', 1)\n",
        "                entry[key] = value.strip('\"')\n",
        "\n",
        "        # Extract patterns\n",
        "        timestamp_match = self.patterns['timestamp'].search(line)\n",
        "        if timestamp_match:\n",
        "            entry['timestamp'] = timestamp_match.group()\n",
        "\n",
        "        severity_match = self.patterns['severity'].search(line)\n",
        "        if severity_match:\n",
        "            entry['severity'] = severity_match.group()\n",
        "\n",
        "        ip_matches = self.patterns['ip'].findall(line)\n",
        "        if ip_matches:\n",
        "            entry['ips'] = ip_matches\n",
        "\n",
        "        return entry\n",
        "\n",
        "    def _update_statistics(self, entry):\n",
        "        \"\"\"Update statistics based on parsed entry\"\"\"\n",
        "        self.statistics['total_entries'] += 1\n",
        "\n",
        "        # Count by severity\n",
        "        if 'severity' in entry:\n",
        "            severity = entry['severity'].upper()\n",
        "            if severity == 'ERROR' or severity == 'CRITICAL':\n",
        "                self.statistics['error_count'] += 1\n",
        "            elif severity == 'WARNING':\n",
        "                self.statistics['warning_count'] += 1\n",
        "\n",
        "        # Count by device (for SCADA logs)\n",
        "        if 'device' in entry:\n",
        "            self.statistics['device_counts'][entry['device']] += 1\n",
        "\n",
        "        # Hourly activity\n",
        "        if 'timestamp' in entry:\n",
        "            try:\n",
        "                hour = entry['timestamp'][:13]  # YYYY-MM-DD HH\n",
        "                self.statistics['hourly_activity'][hour] += 1\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    def detect_anomalies(self, threshold=2):\n",
        "        \"\"\"Detect anomalies based on hourly activity\"\"\"\n",
        "        anomalies = []\n",
        "\n",
        "        if not self.statistics['hourly_activity']:\n",
        "            return anomalies\n",
        "\n",
        "        # Calculate baseline\n",
        "        hourly_counts = list(self.statistics['hourly_activity'].values())\n",
        "        mean = sum(hourly_counts) / len(hourly_counts)\n",
        "\n",
        "        if len(hourly_counts) > 1:\n",
        "            variance = sum((x - mean) ** 2 for x in hourly_counts) / len(hourly_counts)\n",
        "            std_dev = variance ** 0.5\n",
        "            anomaly_threshold = mean + (threshold * std_dev)\n",
        "        else:\n",
        "            anomaly_threshold = mean * 2\n",
        "\n",
        "        # Detect anomalies\n",
        "        for hour, count in self.statistics['hourly_activity'].items():\n",
        "            if count > anomaly_threshold:\n",
        "                anomalies.append({\n",
        "                    'hour': hour,\n",
        "                    'count': count,\n",
        "                    'threshold': anomaly_threshold,\n",
        "                    'deviation': (count - mean) / std_dev if std_dev > 0 else float('inf')\n",
        "                })\n",
        "\n",
        "        return anomalies\n",
        "\n",
        "    def generate_report(self, output_format='text'):\n",
        "        \"\"\"Generate analysis report\"\"\"\n",
        "        report = {\n",
        "            'summary': {\n",
        "                'total_entries': self.statistics['total_entries'],\n",
        "                'error_rate': f\"{(self.statistics['error_count'] / max(self.statistics['total_entries'], 1) * 100):.2f}%\",\n",
        "                'warning_rate': f\"{(self.statistics['warning_count'] / max(self.statistics['total_entries'], 1) * 100):.2f}%\",\n",
        "            },\n",
        "            'device_breakdown': dict(self.statistics['device_counts'].most_common()),\n",
        "            'hourly_activity': dict(self.statistics['hourly_activity']),\n",
        "            'anomalies': self.detect_anomalies()\n",
        "        }\n",
        "\n",
        "        if output_format == 'json':\n",
        "            return json.dumps(report, indent=2)\n",
        "        else:\n",
        "            return self._format_text_report(report)\n",
        "\n",
        "    def _format_text_report(self, report):\n",
        "        \"\"\"Format report as readable text\"\"\"\n",
        "        lines = []\n",
        "        lines.append(\"=\" * 60)\n",
        "        lines.append(\" OT LOG ANALYZER REPORT\")\n",
        "        lines.append(\"=\" * 60)\n",
        "        lines.append(f\"\\n SUMMARY:\")\n",
        "        lines.append(f\"  Total Entries: {report['summary']['total_entries']}\")\n",
        "        lines.append(f\"  Error Rate: {report['summary']['error_rate']}\")\n",
        "        lines.append(f\"  Warning Rate: {report['summary']['warning_rate']}\")\n",
        "\n",
        "        if report['device_breakdown']:\n",
        "            lines.append(f\"\\n DEVICE BREAKDOWN:\")\n",
        "            for device, count in report['device_breakdown'].items():\n",
        "                lines.append(f\"  {device}: {count}\")\n",
        "\n",
        "        if report['anomalies']:\n",
        "            lines.append(f\"\\n ANOMALIES DETECTED:\")\n",
        "            for anomaly in report['anomalies']:\n",
        "                lines.append(f\"  {anomaly['hour']}: {anomaly['count']} events (threshold: {anomaly['threshold']:.0f})\")\n",
        "\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "    def visualize(self):\n",
        "        \"\"\"Create visualization of log activity\"\"\"\n",
        "        if not self.statistics['hourly_activity']:\n",
        "            print(\"No data to visualize\")\n",
        "            return\n",
        "\n",
        "        # Prepare data\n",
        "        hours = sorted(self.statistics['hourly_activity'].keys())\n",
        "        counts = [self.statistics['hourly_activity'][h] for h in hours]\n",
        "\n",
        "        # Create plot\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(range(len(hours)), counts, marker='o', linestyle='-', linewidth=2)\n",
        "        plt.title('OT System Activity Over Time')\n",
        "        plt.xlabel('Time (Hour)')\n",
        "        plt.ylabel('Number of Log Events')\n",
        "        plt.xticks(range(len(hours)), [h[-5:] for h in hours], rotation=45)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Highlight anomalies\n",
        "        anomalies = self.detect_anomalies()\n",
        "        for anomaly in anomalies:\n",
        "            if anomaly['hour'] in hours:\n",
        "                idx = hours.index(anomaly['hour'])\n",
        "                plt.plot(idx, anomaly['count'], 'ro', markersize=10)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    # Initialize analyzer\n",
        "    analyzer = OTLogAnalyzer()\n",
        "\n",
        "    # Create a sample log file for testing\n",
        "    sample_log = \"\"\"\n",
        "2026-02-19 08:23:45 PLC INFO System startup complete\n",
        "2026-02-19 08:23:46 PLC INFO Connection established to SCADA\n",
        "2026-02-19 08:24:12 Firewall INFO Connection from 192.168.1.100 allowed\n",
        "2026-02-19 08:25:33 RTU WARNING Communication timeout, retry 1/3\n",
        "2026-02-19 08:26:45 PLC INFO Normal operation mode\n",
        "2026-02-19 09:15:22 Firewall ERROR Unauthorized access attempt from 10.0.0.50\n",
        "2026-02-19 09:16:01 PLC CRITICAL Unexpected shutdown detected\n",
        "2026-02-19 09:16:45 HMI INFO Operator logged in\n",
        "    \"\"\".strip().split('\\n')\n",
        "\n",
        "    # Write sample to file\n",
        "    with open('sample_ot.log', 'w') as f:\n",
        "        for line in sample_log:\n",
        "            f.write(line + '\\n')\n",
        "\n",
        "    # Analyze the log\n",
        "    analyzer.parse_log('sample_ot.log')\n",
        "\n",
        "    # Generate report\n",
        "    print(analyzer.generate_report())\n",
        "\n",
        "    # Visualize (uncomment to see plot)\n",
        "    # analyzer.visualize()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}